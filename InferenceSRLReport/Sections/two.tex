\section{Build an Inference Engine: Pipeline}

The pipeline has been implemented in Python3. The README is in the same directory as the code and will explain how the code should be run. Some computations have been carried out in miniC2D. The results are shown in tables~\ref{table:results-task11} to~\ref{table:results-bayesian}.
\\\\
We use ENC2, as specified in the first task, as our encoding. We chose this because it minimizes the amount of variables needed to get a full CNF. The encoding as we implemented it, is not always very efficient. In the case of task 12 we have more variables than actually needed. Table~\ref{table:results-task12} shows that we need 16 variables and 28 clauses to represent the CNF for \texttt{query(path15)}. In the CNF folder where, where we manually created the CNFs, we only need 6 variables and 8 clauses. The difference can be explained as we create two lambda variables and a rho variable for every atom in ENC2. However, edges are never used in their negation in clauses. This means that there are no rules that express \texttt{\\+edge(1,3)} for example. The lambda that expresses \textit{not} an edge and the corresponding $\rho$ are actually useless. A possible way to fix this is to scan over all the rules and ensure that the negation never occurs (or that only the negation occurs). 

\begin{table}[h]
\centering
\caption{Results task11}
\label{table:results-task11}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
\textbf{query} & \textbf{probability} & \textbf{count time (s)} & \textbf{vtree time (s)} & \textbf{CNF time (s)} & \textbf{\#vars CNF} & \textbf{\#clauses CNF} & \textbf{depth vtree} & \textbf{branching factor vtree} & \textbf{\#edges circuit} & \textbf{\#nodes circuit} & \textbf{total runtime (s)} \\ \hline
burglary &  $\frac{0.364}{0.372} = 0.98$ & 0.006 & 0.001  &  0.000 &  24 & 37 &   &  &  &  &  0.021 \\
e(none) & $\frac{0.290}{0.371} = 0.78$  & 0.005 & 0.000 &  0.000 & 24 & 37 &  &    &   &  & 0.020
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{Results task12}
\label{table:results-task12}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
\textbf{query} & \textbf{probability} & \textbf{count time (s)} & \textbf{vtree time (s)} & \textbf{CNF time (s)} & \textbf{\#vars CNF} & \textbf{\#clauses CNF} & \textbf{depth vtree} & \textbf{branching factor vtree} & \textbf{\#edges circuit} & \textbf{\#nodes circuit} & \textbf{total runtime (s)} \\ \hline
path15 &  0.258  & 0.000 & 0.000 & 0.000 & 16 & 28 &  &  &  &   &  0.015 \\
path16 & 0.217 &  0.002  & 0.001 & 0.000 & 22 & 55 &  &  &  &   &  0.017
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{Results task Bayesian network}
\label{table:results-bayesian}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
\textbf{query} & \textbf{probability} & \textbf{count time (s)} & \textbf{vtree time (s)} & \textbf{CNF time (s)} & \textbf{\#vars CNF} & \textbf{\#clauses CNF} & \textbf{depth vtree} & \textbf{branching factor vtree} & \textbf{\#edges circuit} & \textbf{\#nodes circuit} & \textbf{total runtime (s)} \\ \hline
hREKG(LOW) & 0.178  &  0.039  & 0.015  & 0.001  & 319 & 482      &   &   &    &     & 0.070  \\
pRESS(HIGH) &  0.484  &  0.008  & 0.003 & 0.000  & 83 & 130  &  &   &       &    &   0.026 \\
aRTCO2(LOW) &  0.193 &  0.023 & 0.006 & 0.000  & 179 & 248 &  &  &  &  & 0.045  \\
sAO2(LOW) & 0.770  & 0.027 & 0.007 & 0.000 & 214 & 302 &   &  &  &  &  0.050
\end{tabular}
\end{table}

