\section{Probabilistic Inference Using Weighted Model Counting}
\subsection{PGM to CNF}

Table~\ref{variableDescription} shows the semantics of the domain variables used for those tasks.

Tables~\ref{logicalEnc1} and~\ref{logicalEnc2} show the logical variables used for encoding the Bayesian Network.

Table~\ref{cnfRepresentationEnc1} represents the encoded Bayesian Network using ENC1 and table~\ref{weightsEnc1} contains the corresponding weights.

Likewise, table~\ref{cnfRepresentationEnc2} represents the encoded Bayesian Network using ENC2 and table~\ref{weightsEnc2} contains the corresponding weights.

\input{Tables/variableDescription}
\input{Tables/logicalEnc1}
\input{Tables/logicalEnc2}

\input{Tables/cnfRepresentationEnc1}
\input{Tables/weightsEnc1}
\input{Tables/cnfRepresentationEnc1Expanded}

\input{Tables/cnfRepresentationEnc2}
\input{Tables/weightsEnc2}
\input{Tables/cnfRepresentationEnc2_expand}

\subsection{SRL to CNF}

First the program must be grounded, while taking into account \textbf{Q} and \textbf{E}. In this case the evidence set \textbf{E} is empty (there is no evidence available). The grounding process of the queries will be described step-by-step in listings~\ref{lst:query15} and~\ref{lst:query16}.  If only \textbf{query(path(1,5))} was considered, then \textbf{edge(5,6)} and \textbf{edge(2,6)} would have been irrelevant. With the inclusion of \textbf{query(path(1,6))} all edges become relevant.

\input{Listings/12_querygrounding}

The second step is to find an equivalent CNF of the ground program. Given the grounded rules \mbox{\textbf{w :- r}} and \mbox{\textbf{w :- s}}, the equivalent CNF contains the following three clauses: $\neg r \lor w$, $\neg s \lor w$ and $\neg w \lor s \lor r$. In our case $r$ and $s$ both are conjunctions, so De Morgans law is used to write the first two clauses. For the last clause, all permutations of the combinations of the elements $\neg w$, $r$ and $s$ are considered. For \textbf{path(1,5)} this yields $2*3 = 6$ combinations. For \textbf{path(1,6)} there are $2*3*4 = 24$ combinations. The CNF is shown in table~\ref{table:12cnf}. Note that on the last big block of $path_{16}$ that the \textit{and} operators can be removed, and the separate clauses can be listed underneath each other. We chose to use the current format because the resulting table would become too large (vertically) otherwise. It also clearly shows which clauses corresponds to the 24 combinations.

\input{Tables/12_cnf}

The final step is to obtain a weighted CNF. Since there's no evidence in our example, the CNF remains the same as shown in table~\ref{table:12cnf}. Table~\ref{table:12weight} displays the weighted literals. The weights for $path_{15}$, $path_{16}$, $\neg path_{15}$ and $\neg path_{16}$ equal 1 because they're defined in clauses. The weight of any world $\omega$ can be calculated as the product of the weight of all literals in $\omega$. For example, the world ${path_{15}, edge_{12}, edge_{25}, edge_{13}, edge_{34}, \neg edge_{45}}$ has the weight $0.6*0.4*0.1*0.3*0.2 = 0.00144$.

\input{Tables/12_weights}


\subsection{Weighted Model Counting}

We used the following exact model counters: \textbf{MiniC2D}, \textbf{SDD} and \textbf{sharpSAT}.
\\[2ex]
MiniC2D uses exhaustive DPLL, a backtracking based search algorithm to solve the \textit{boolean satisfiability problem}. It compiles CNFs into Decision-SDDs for the knowledge compilation, using a top-down approach. The top-down approach is considered to be faster by \textit{Umut Oztok and Adnan Darwiche}\cite{oztok2015top}. The Decision-SDDs are a subset of SDDs which facilitate the top-down compilation of SDDs.
\\[2ex]
The SDD program is similar to MiniC2D in the sense that it compiles CNFs into SDD datastructures. The shape of the SDD can be manipulated to improve efficiency, as will be explained in section~\ref{subsec:knowledgecompilation}.
\\[2ex]
sharpSAT also uses DPLL, but combines this with \textit{look ahead} technique that is based on \textit{boolean constraint propagation}~\cite{thurley2006sharpsat}. The look ahead technique eliminates more variables that can not be part of a solution, and thus reduces the search space to backtrack over.
\\[2ex]
Table~\ref{table:computationalrequirements} shows the computational requirements of each solver on the CNF from task 1, using encoding 1 and query \textbf{burglary}. There are 36 variables 94 clauses. \textit{The CNF encodings are included with our program.}

\begin{table}[h]
	\centering
	\caption{Computational requirements.}
	\label{table:computationalrequirements}
	\begin{tabular}{l|l|l|l}
		\textbf{}        & \textbf{miniC2D} & \textbf{SDD} & \textbf{sharpSAT} \\ \hline
		\textbf{total runtime} &  0.019s  &   0.044s   &   0.008s  \\ \hline
		\textbf{memory (cache size)}  &  0.011MB  &   0.3MB &   7MB  \\ \hline
		\textbf{cache hit rate}  & 66.7\%  & 85.2\% &   100\%
	\end{tabular}
\end{table}

\subsection{Knowledge Compilation}\label{subsec:knowledgecompilation}
